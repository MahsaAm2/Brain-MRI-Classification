{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:27:20.078796Z",
     "iopub.status.busy": "2024-09-02T07:27:20.078373Z",
     "iopub.status.idle": "2024-09-02T07:27:25.429681Z",
     "shell.execute_reply": "2024-09-02T07:27:25.428397Z",
     "shell.execute_reply.started": "2024-09-02T07:27:20.078755Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import fastai\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/kaggle/input/iaaa-mri-challenge/train.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('/kaggle/working/separated_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_data_folder = '/kaggle/working/separated_data'\n",
    "data_folder = '/kaggle/input/iaaa-mri-challenge/data'\n",
    "\n",
    "normal_folder = os.path.join(separated_data_folder, 'normal')\n",
    "abnormal_folder = os.path.join(separated_data_folder, 'abnormal')\n",
    "\n",
    "os.makedirs(normal_folder, exist_ok=True)\n",
    "os.makedirs(abnormal_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the CSV file and move folders\n",
    "for index, row in df.iterrows():\n",
    "    series_uid = row['SeriesInstanceUID']\n",
    "    prediction = row['prediction']\n",
    "    \n",
    "    # Determine the source and destination paths\n",
    "    src_folder = os.path.join(data_folder, series_uid)\n",
    "    if prediction == 0:\n",
    "        dst_folder = os.path.join(normal_folder, series_uid)\n",
    "    else:\n",
    "        dst_folder = os.path.join(abnormal_folder, series_uid)\n",
    "    \n",
    "    # Move the folder\n",
    "    if os.path.exists(src_folder):\n",
    "        shutil.copytree(src_folder, dst_folder)\n",
    "    else:\n",
    "        print(f\"Folder {src_folder} does not exist\")\n",
    "\n",
    "print(\"Folders separated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('/kaggle/working/splited_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the normal and abnormal folders\n",
    "\n",
    "base_folder = '/kaggle/working/splited_dataset'\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Create directories for train, val, and test sets\n",
    "train_folder = os.path.join(base_folder, 'train')\n",
    "val_folder = os.path.join(base_folder, 'val')\n",
    "test_folder = os.path.join(base_folder, 'test')\n",
    "\n",
    "for subset in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(train_folder, 'normal'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(train_folder, 'abnormal'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_folder, 'normal'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_folder, 'abnormal'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_folder, 'normal'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_folder, 'abnormal'), exist_ok=True)\n",
    "\n",
    "# Function to split and move folders\n",
    "def split_and_move_folders(src_folder, dst_base_folder):\n",
    "    folders = [f for f in os.listdir(src_folder) if os.path.isdir(os.path.join(src_folder, f))]\n",
    "    random.shuffle(folders)\n",
    "    \n",
    "    train_count = int(len(folders) * train_ratio)\n",
    "    val_count = int(len(folders) * val_ratio)\n",
    "    test_count = len(folders) - train_count - val_count\n",
    "    \n",
    "    splits = {\n",
    "        'train': folders[:train_count],\n",
    "        'val': folders[train_count:train_count + val_count],\n",
    "        'test': folders[train_count + val_count:]\n",
    "    }\n",
    "    \n",
    "    for split, split_folders in splits.items():\n",
    "        for folder in split_folders:\n",
    "            src_path = os.path.join(src_folder, folder)\n",
    "            dst_path = os.path.join(dst_base_folder, split, os.path.basename(src_folder), folder)\n",
    "            shutil.move(src_path, dst_path)\n",
    "\n",
    "# Split and move normal and abnormal folders\n",
    "split_and_move_folders(normal_folder, base_folder)\n",
    "split_and_move_folders(abnormal_folder, base_folder)\n",
    "\n",
    "print(\"Data split into train, val, and test sets successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('/kaggle/working/final_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_dicom(src_folder, dst_folder):\n",
    "    for root, _, files in os.walk(src_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                dicom_path = os.path.join(root, file)\n",
    "                dicom_data = pydicom.dcmread(dicom_path)\n",
    "                \n",
    "                # Extract metadata\n",
    "                slice_orientation = dicom_data[(0x2001, 0x100b)].value\n",
    "                if slice_orientation != 'SAGITTAL':\n",
    "                    pixel_array = dicom_data.pixel_array\n",
    "                    protocol_name = dicom_data.ProtocolName.replace(\" \", \"_\")  # Remove spaces in description\n",
    "                    instance_number = dicom_data.InstanceNumber\n",
    "\n",
    "                    # Get the original folder name (parent of current root)\n",
    "                    original_folder_name = os.path.basename(root)\n",
    "\n",
    "                    # Create a new folder for the subject using original folder name + PatientID\n",
    "                    subject_folder_name = f\"{original_folder_name}\"\n",
    "                    subject_folder = os.path.join(dst_folder, subject_folder_name)\n",
    "                    os.makedirs(subject_folder, exist_ok=True)\n",
    "\n",
    "                    # Construct the save path with instance number in the filename\n",
    "                    save_path = os.path.join(subject_folder, f\"{os.path.splitext(file)[0]}_{protocol_name}_Instance{instance_number}.npy\")\n",
    "\n",
    "                    # Save the numpy array\n",
    "              \n",
    "                    np.save(save_path, pixel_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/kaggle/working/final_data'\n",
    "\n",
    "# Create output directories for train, val, and test sets\n",
    "for subset in ['train', 'val', 'test']:\n",
    "    for category in ['normal', 'abnormal']:\n",
    "        os.makedirs(os.path.join(output_folder, subset, category), exist_ok=True)\n",
    "\n",
    "# Process and save DICOM files in train, val, and test sets\n",
    "for subset in ['train', 'val', 'test']:\n",
    "    for category in ['normal', 'abnormal']:\n",
    "        src_folder = os.path.join(base_folder, subset, category)\n",
    "        dst_folder = os.path.join(output_folder, subset, category)\n",
    "        process_and_save_dicom(src_folder, dst_folder)\n",
    "\n",
    "print(\"DICOM files processed and saved as NumPy arrays with SeriesDescription in filenames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_instance_number(filename):\n",
    "    \"\"\"Extract the instance number from the filename.\"\"\"\n",
    "    # Assuming the instance number is always preceded by 'Instance'\n",
    "    base_name = os.path.splitext(os.path.basename(filename))[0]\n",
    "    instance_str = base_name.split('_')[-1]  # Get the last part after splitting by '_'\n",
    "    \n",
    "    # Remove any non-digit characters from the instance part to extract the number\n",
    "    instance_number = ''.join(filter(str.isdigit, instance_str))\n",
    "    \n",
    "    return int(instance_number)\n",
    "\n",
    "def show_all_slices_of_random_subject(output_folder):\n",
    "    # List all subject folders in the output directory\n",
    "    subject_folders = [f.path for f in os.scandir(output_folder) if f.is_dir()]\n",
    "    \n",
    "    if not subject_folders:\n",
    "        print(\"No subjects found in the output folder.\")\n",
    "        return\n",
    "    \n",
    "    # Select a random subject folder\n",
    "    random_subject_folder = random.choice(subject_folders)\n",
    "    print(f\"Displaying all slices for subject: {os.path.basename(random_subject_folder)}\")\n",
    "    \n",
    "    # Get all .npy files in the subject's folder\n",
    "    npy_files = [os.path.join(random_subject_folder, f) for f in os.listdir(random_subject_folder) if f.endswith(\".npy\")]\n",
    "    \n",
    "    if not npy_files:\n",
    "        print(\"No .npy files found for the selected subject.\")\n",
    "        return\n",
    "    \n",
    "    # Sort files based on extracted instance numbers\n",
    "    npy_files = sorted(npy_files, key=extract_instance_number)\n",
    "\n",
    "    # Number of slices\n",
    "    num_slices = len(npy_files)\n",
    "    \n",
    "    # Determine grid size (rows and columns) for displaying the slices\n",
    "    num_columns = 5  # Display 5 images per row\n",
    "    num_rows = (num_slices + num_columns - 1) // num_columns  # Calculate the number of rows needed\n",
    "    \n",
    "    # Create a figure with the calculated grid size\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 3))\n",
    "    axes = axes.flatten()  # Flatten the axes array\n",
    "    \n",
    "    # Display each slice in the grid\n",
    "    for i, file_path in enumerate(npy_files):\n",
    "        # Load the image data from the .npy file\n",
    "        image_data = np.load(file_path)\n",
    "        \n",
    "        # Extract the instance number using the defined function\n",
    "        instance_number = extract_instance_number(file_path)\n",
    "        \n",
    "        # Display the image and set the title as the instance number\n",
    "        axes[i].imshow(image_data, cmap='gray')\n",
    "        axes[i].set_title(f\"Instance {instance_number}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the base output folder (adjust based on your folder structure)\n",
    "normal_folder = '/kaggle/working/final_data/train/normal'  # Change this to the relevant path\n",
    "\n",
    "# Show a random subject's images\n",
    "show_all_slices_of_random_subject(normal_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the base output folder (adjust based on your folder structure)\n",
    "abnormal_folder = '/kaggle/working/final_data/train/abnormal'  # Change this to the relevant path\n",
    "\n",
    "# Show a random subject's images\n",
    "show_all_slices_of_random_subject(abnormal_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject_slices(subject_folder):\n",
    "    \"\"\"Process the slices of a subject, resize them, split into hemispheres, and return the array.\"\"\"\n",
    "    \n",
    "    # Get all .npy files in the subject's folder\n",
    "    npy_files = [os.path.join(subject_folder, f) for f in os.listdir(subject_folder) if f.endswith(\".npy\")]\n",
    "    \n",
    "    if not npy_files:\n",
    "        print(\"No .npy files found in the selected subject folder.\")\n",
    "        return None\n",
    "    \n",
    "    # Sort files based on extracted instance numbers\n",
    "    npy_files = sorted(npy_files, key=extract_instance_number)\n",
    "    \n",
    "    # Limit to the first 16 slices, or pad with zeros if there are fewer\n",
    "    num_slices = 16\n",
    "    processed_slices = []\n",
    "    \n",
    "    for i in range(num_slices):\n",
    "        if i < len(npy_files):\n",
    "            image_data = np.load(npy_files[i])\n",
    "        else:\n",
    "            # Create a zero array if there are not enough slices\n",
    "            image_data = np.zeros((512, 512))  # Assuming original slices are 512x512\n",
    "        \n",
    "        # Resize to 256x256\n",
    "        resized_slice = cv2.resize(image_data, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Split into left and right hemispheres\n",
    "        left_hemisphere = resized_slice[:, :128]\n",
    "        right_hemisphere = resized_slice[:, 128:]\n",
    "        \n",
    "        # Append hemispheres to the processed_slices list\n",
    "        processed_slices.append([left_hemisphere, right_hemisphere])\n",
    "    \n",
    "    # Convert to numpy array with shape (16, 2, 256, 128)\n",
    "    processed_slices = np.array(processed_slices)\n",
    "    \n",
    "    return processed_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_processed_slices(processed_slices):\n",
    "    \"\"\"Display the processed slices with left and right hemispheres.\"\"\"\n",
    "    if processed_slices is None:\n",
    "        print(\"No slices to display.\")\n",
    "        return\n",
    "    \n",
    "    num_slices = processed_slices.shape[0]\n",
    "    fig, axes = plt.subplots(num_slices, 2, figsize=(10, num_slices * 2.5))\n",
    "    \n",
    "    for i in range(num_slices):\n",
    "        # Display left hemisphere\n",
    "        axes[i, 0].imshow(processed_slices[i, 0], cmap='gray')\n",
    "        axes[i, 0].set_title(f'Slice {i+1} - Left Hemisphere')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Display right hemisphere\n",
    "        axes[i, 1].imshow(processed_slices[i, 1], cmap='gray')\n",
    "        axes[i, 1].set_title(f'Slice {i+1} - Right Hemisphere')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_folder = '/kaggle/working/final_data/train/abnormal/1.3.46.670589.11.10042.5.0.6596.2024012113454106397'  # Adjust the path as needed\n",
    "processed_slices = process_subject_slices(subject_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the processed slices\n",
    "show_processed_slices(processed_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_for_subject(subject_folder):\n",
    "    # Get all .npy files in the subject's folder\n",
    "    npy_files = [os.path.join(subject_folder, f) for f in os.listdir(subject_folder) if f.endswith(\".npy\")]\n",
    "    \n",
    "    if not npy_files:\n",
    "        print(\"No .npy files found for the selected subject.\")\n",
    "        return\n",
    "    \n",
    "    # Sort files to maintain order (assuming filenames include instance numbers)\n",
    "    npy_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[-1].replace(\"Instance\", \"\")))\n",
    "    \n",
    "    for i, file_path in enumerate(npy_files):\n",
    "        # Load the image data from the .npy file\n",
    "        image_data = np.load(file_path)\n",
    "        \n",
    "        # Resize the image to 256x256\n",
    "        resized_image = cv2.resize(image_data, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Split the image into left and right hemispheres\n",
    "        left_hemisphere = resized_image[:, :128]\n",
    "        right_hemisphere = resized_image[:, 128:]\n",
    "        \n",
    "        # Exclude zeros from histograms\n",
    "        left_hemisphere_nonzero = left_hemisphere[left_hemisphere > 0]\n",
    "        right_hemisphere_nonzero = right_hemisphere[right_hemisphere > 0]\n",
    "        \n",
    "        # Plot histograms\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        if left_hemisphere_nonzero.size > 0:  # Ensure there is non-zero data to plot\n",
    "            axes[0].hist(left_hemisphere_nonzero.ravel(), bins=255, color='blue', alpha=0.7)\n",
    "        axes[0].set_title(f'Slice {i+1} - Left Hemisphere')\n",
    "        axes[0].set_xlabel('Pixel Intensity')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "\n",
    "        if right_hemisphere_nonzero.size > 0:  # Ensure there is non-zero data to plot\n",
    "            axes[1].hist(right_hemisphere_nonzero.ravel(), bins=255, color='green', alpha=0.7)\n",
    "        axes[1].set_title(f'Slice {i+1} - Right Hemisphere')\n",
    "        axes[1].set_xlabel('Pixel Intensity')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "\n",
    "        plt.suptitle(f\"Histograms for Slice {i+1} of Subject {os.path.basename(subject_folder)}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_folder = '/kaggle/working/final_data/train/abnormal/1.3.46.670589.11.10042.5.0.6596.2024012113454106397'  # Replace with the actual path to the subject's folder\n",
    "plot_histograms_for_subject(abnormal_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_folder = '/kaggle/working/final_data/train/normal/1.3.46.670589.11.10042.5.0.6596.2024012100305431405'  # Replace with the actual path to the subject's folder\n",
    "plot_histograms_for_subject(normal_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:27:46.713943Z",
     "iopub.status.busy": "2024-09-02T07:27:46.712659Z",
     "iopub.status.idle": "2024-09-02T07:27:46.727626Z",
     "shell.execute_reply": "2024-09-02T07:27:46.726256Z",
     "shell.execute_reply.started": "2024-09-02T07:27:46.713883Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_histograms_for_subject(subject_folder, num_bins=255):\n",
    "    # Get all .npy files in the subject's folder\n",
    "    npy_files = [os.path.join(subject_folder, f) for f in os.listdir(subject_folder) if f.endswith(\".npy\")]\n",
    "    \n",
    "    if not npy_files:\n",
    "        print(f\"No .npy files found for the subject folder: {subject_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # Sort files to maintain order (assuming filenames include instance numbers)\n",
    "    npy_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[-1].replace(\"Instance\", \"\")))\n",
    "\n",
    "    all_histograms = []\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        # Load the image data from the .npy file\n",
    "        image_data = np.load(file_path)\n",
    "        \n",
    "        # Resize the image to 256x256\n",
    "        resized_image = cv2.resize(image_data, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Split the image into left and right hemispheres\n",
    "        left_hemisphere = resized_image[:, :128]\n",
    "        right_hemisphere = resized_image[:, 128:]\n",
    "        \n",
    "        # Exclude zeros and compute histograms\n",
    "        left_hist, _ = np.histogram(left_hemisphere[left_hemisphere > 0], bins=num_bins, range=(0, 255))\n",
    "        right_hist, _ = np.histogram(right_hemisphere[right_hemisphere > 0], bins=num_bins, range=(0, 255))\n",
    "        \n",
    "        # If no non-zero pixels are found, skip this slice\n",
    "        if left_hist.sum() == 0 and right_hist.sum() == 0:\n",
    "            print(f\"Skipping slice with no non-zero pixels in file: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Combine histograms for left and right hemispheres\n",
    "        combined_hist = np.concatenate([left_hist, right_hist])\n",
    "        \n",
    "        # Normalize histogram to ensure equal contribution of each slice\n",
    "        if combined_hist.sum() > 0:\n",
    "            combined_hist = combined_hist / np.sum(combined_hist)\n",
    "        \n",
    "        all_histograms.append(combined_hist)\n",
    "    \n",
    "    # If no valid histograms were found, return None\n",
    "    if not all_histograms:\n",
    "        print(f\"No valid histograms found for the subject folder: {subject_folder}\")\n",
    "        return None\n",
    "    \n",
    "    # If there are more than 16 slices, take the first 16\n",
    "    # If there are fewer than 16 slices, pad with zeros\n",
    "    num_slices = len(all_histograms)\n",
    "    if num_slices < 16:\n",
    "        all_histograms.extend([np.zeros(2 * num_bins)] * (16 - num_slices))\n",
    "    elif num_slices > 16:\n",
    "        all_histograms = all_histograms[:16]\n",
    "    \n",
    "    # Flatten the list of histograms into a single feature vector\n",
    "    subject_histogram_vector = np.concatenate(all_histograms)\n",
    "    \n",
    "    return subject_histogram_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:27:52.513455Z",
     "iopub.status.busy": "2024-09-02T07:27:52.512966Z",
     "iopub.status.idle": "2024-09-02T07:27:53.863448Z",
     "shell.execute_reply": "2024-09-02T07:27:53.862181Z",
     "shell.execute_reply.started": "2024-09-02T07:27:52.513409Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:27:56.972753Z",
     "iopub.status.busy": "2024-09-02T07:27:56.972093Z",
     "iopub.status.idle": "2024-09-02T07:27:56.985080Z",
     "shell.execute_reply": "2024-09-02T07:27:56.983640Z",
     "shell.execute_reply.started": "2024-09-02T07:27:56.972699Z"
    }
   },
   "outputs": [],
   "source": [
    "class BrainHistogramDataset(Dataset):\n",
    "    def __init__(self, base_folder, num_bins=255):\n",
    "        self.num_bins = num_bins\n",
    "        \n",
    "        # List all subject folders in both 'normal' and 'abnormal' directories\n",
    "        normal_folder = os.path.join(base_folder, 'normal')\n",
    "        abnormal_folder = os.path.join(base_folder, 'abnormal')\n",
    "        \n",
    "        self.subject_folders = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Add normal subjects\n",
    "        normal_subject_folders = [os.path.join(normal_folder, f) for f in os.listdir(normal_folder) if os.path.isdir(os.path.join(normal_folder, f))]\n",
    "        self.subject_folders.extend(normal_subject_folders)\n",
    "        self.labels.extend([0] * len(normal_subject_folders))  # Label 0 for normal\n",
    "     \n",
    "        \n",
    "        # Add abnormal subjects\n",
    "        abnormal_subject_folders = [os.path.join(abnormal_folder, f) for f in os.listdir(abnormal_folder) if os.path.isdir(os.path.join(abnormal_folder, f))]\n",
    "        self.subject_folders.extend(abnormal_subject_folders)\n",
    "        self.labels.extend([1] * len(abnormal_subject_folders))  # Label 1 for abnormal\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subject_folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_folder = self.subject_folders[idx]\n",
    "        histogram_vector = compute_histograms_for_subject(subject_folder, self.num_bins)\n",
    "        \n",
    "        # Handle cases where histogram_vector is None\n",
    "        if histogram_vector is None:\n",
    "            histogram_vector = np.zeros(16 * 2 * self.num_bins)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(histogram_vector, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:28:01.060416Z",
     "iopub.status.busy": "2024-09-02T07:28:01.059826Z",
     "iopub.status.idle": "2024-09-02T07:28:01.066540Z",
     "shell.execute_reply": "2024-09-02T07:28:01.064806Z",
     "shell.execute_reply.started": "2024-09-02T07:28:01.060357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paths to your data folders\n",
    "train_folder = '/kaggle/working/final_data/train'\n",
    "val_folder = '/kaggle/working/final_data/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:28:04.433249Z",
     "iopub.status.busy": "2024-09-02T07:28:04.432804Z",
     "iopub.status.idle": "2024-09-02T07:28:04.532057Z",
     "shell.execute_reply": "2024-09-02T07:28:04.530859Z",
     "shell.execute_reply.started": "2024-09-02T07:28:04.433210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = BrainHistogramDataset(train_folder)\n",
    "val_dataset = BrainHistogramDataset(val_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:28:08.197095Z",
     "iopub.status.busy": "2024-09-02T07:28:08.196635Z",
     "iopub.status.idle": "2024-09-02T07:28:08.202661Z",
     "shell.execute_reply": "2024-09-02T07:28:08.201283Z",
     "shell.execute_reply.started": "2024-09-02T07:28:08.197050Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:28:11.340139Z",
     "iopub.status.busy": "2024-09-02T07:28:11.339235Z",
     "iopub.status.idle": "2024-09-02T07:28:11.347573Z",
     "shell.execute_reply": "2024-09-02T07:28:11.346237Z",
     "shell.execute_reply.started": "2024-09-02T07:28:11.340079Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_sample_weights(train_dataset):\n",
    "    # Extract labels from the dataset\n",
    "    labels = [label for _, label in train_dataset]\n",
    "    \n",
    "    # Calculate the number of samples per class\n",
    "    class_sample_counts = np.bincount(labels)\n",
    "    \n",
    "    # Compute class weights as the inverse of the class sample counts\n",
    "    class_weights = 1.0 / (class_sample_counts + 1e-9)  # Adding a small value to avoid division by zero\n",
    "    \n",
    "    # Normalize class weights (optional)\n",
    "    class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "    print(class_weights)\n",
    "    \n",
    "    # Create sample weights based on class weights\n",
    "    sample_weights = np.array([class_weights[label] for label in labels])\n",
    "    \n",
    "    # Scale the weights if they are too small\n",
    "    sample_weights *= len(sample_weights)  # Scale by the number of samples\n",
    "    \n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:28:15.095917Z",
     "iopub.status.busy": "2024-09-02T07:28:15.095447Z",
     "iopub.status.idle": "2024-09-02T07:29:38.815594Z",
     "shell.execute_reply": "2024-09-02T07:29:38.814374Z",
     "shell.execute_reply.started": "2024-09-02T07:28:15.095875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24759615 1.75240385]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sample_weights = compute_sample_weights(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:29:53.207932Z",
     "iopub.status.busy": "2024-09-02T07:29:53.207441Z",
     "iopub.status.idle": "2024-09-02T07:29:53.214043Z",
     "shell.execute_reply": "2024-09-02T07:29:53.212702Z",
     "shell.execute_reply.started": "2024-09-02T07:29:53.207891Z"
    }
   },
   "outputs": [],
   "source": [
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:29:57.392597Z",
     "iopub.status.busy": "2024-09-02T07:29:57.392126Z",
     "iopub.status.idle": "2024-09-02T07:29:57.399696Z",
     "shell.execute_reply": "2024-09-02T07:29:57.398017Z",
     "shell.execute_reply.started": "2024-09-02T07:29:57.392553Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:30:01.042015Z",
     "iopub.status.busy": "2024-09-02T07:30:01.041556Z",
     "iopub.status.idle": "2024-09-02T07:30:01.047683Z",
     "shell.execute_reply": "2024-09-02T07:30:01.046525Z",
     "shell.execute_reply.started": "2024-09-02T07:30:01.041972Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:30:04.849097Z",
     "iopub.status.busy": "2024-09-02T07:30:04.848679Z",
     "iopub.status.idle": "2024-09-02T07:30:04.856717Z",
     "shell.execute_reply": "2024-09-02T07:30:04.855596Z",
     "shell.execute_reply.started": "2024-09-02T07:30:04.849058Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:44:18.944253Z",
     "iopub.status.busy": "2024-09-02T07:44:18.943233Z",
     "iopub.status.idle": "2024-09-02T07:44:18.955700Z",
     "shell.execute_reply": "2024-09-02T07:44:18.954439Z",
     "shell.execute_reply.started": "2024-09-02T07:44:18.944207Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, model_save_path='best_mlp_model.pth'):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Smaller learning rate\n",
    "    best_auc_pr = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze(1)  # Ensure outputs are of shape (batch_size,)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "                outputs = model(inputs).squeeze(1)  # Ensure outputs are of shape (batch_size,)\n",
    "                \n",
    "                preds = (outputs > 0.5).cpu().numpy()  # Binarize predictions\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        precision = precision_score(all_labels, all_preds, zero_division=1)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        auc_score = roc_auc_score(all_labels, all_preds)\n",
    "        auc_pr = average_precision_score(all_labels, all_preds)\n",
    "\n",
    "        print(f'Validation - Precision: {precision:.4f}, Recall: {recall:.4f}, AUC: {auc_score:.4f}, AUC-PR: {auc_pr:.4f}')\n",
    "\n",
    "        # Save model if it's the best so far\n",
    "        if auc_pr > best_auc_pr:\n",
    "            best_auc_pr = auc_pr\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'Saved best model with AUC-PR: {best_auc_pr:.4f}')\n",
    "\n",
    "    print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T07:44:22.889275Z",
     "iopub.status.busy": "2024-09-02T07:44:22.888837Z",
     "iopub.status.idle": "2024-09-02T07:44:22.895179Z",
     "shell.execute_reply": "2024-09-02T07:44:22.893835Z",
     "shell.execute_reply.started": "2024-09-02T07:44:22.889234Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input size based on histogram size (16 slices * 2 hemispheres * 50 bins)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = 16 * 2 * 255\n",
    "model = MLPClassifier(input_size=input_size)\n",
    "\n",
    "# Train the model\n",
    "best_model = train_model(model, train_loader, val_loader, model_save_path='best_mlp_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T09:35:14.205752Z",
     "iopub.status.busy": "2024-09-02T09:35:14.204793Z",
     "iopub.status.idle": "2024-09-02T09:35:14.234555Z",
     "shell.execute_reply": "2024-09-02T09:35:14.233402Z",
     "shell.execute_reply.started": "2024-09-02T09:35:14.205696Z"
    }
   },
   "outputs": [],
   "source": [
    "test_folder = '/kaggle/working/final_data/test'\n",
    "test_dataset = BrainHistogramDataset(test_folder)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T09:39:08.657154Z",
     "iopub.status.busy": "2024-09-02T09:39:08.656128Z",
     "iopub.status.idle": "2024-09-02T09:39:08.693885Z",
     "shell.execute_reply": "2024-09-02T09:39:08.692456Z",
     "shell.execute_reply.started": "2024-09-02T09:39:08.657107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(\n",
       "  (fc1): Linear(in_features=8160, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = MLPClassifier(input_size=input_size)\n",
    "model_test.load_state_dict(torch.load('best_mlp_model.pth' ,weights_only=True))\n",
    "model_test.to(device)\n",
    "model_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "        outputs = model_test(inputs).squeeze(1)\n",
    "        \n",
    "        preds = (outputs > 0.5).cpu().numpy()  # Binarize predictions\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for metric calculations\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(all_labels, all_preds, zero_division=1)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "auc_score = roc_auc_score(all_labels, all_preds)\n",
    "auc_pr = average_precision_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Test - Precision: {precision:.4f}, Recall: {recall:.4f}, AUC: {auc_score:.4f}, AUC-PR: {auc_pr:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5372944,
     "sourceId": 8931416,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
